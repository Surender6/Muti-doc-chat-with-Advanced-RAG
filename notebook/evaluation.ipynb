{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74de8af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1e243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#QA\n",
    "inputs = [\n",
    "    \"For customer-facing applications, which company's models dominate the top rankings?\",\n",
    "    \"What percentage of respondents are using RAG in some form?\",\n",
    "    \"How often are most respondents updating their models?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"OpenAI models dominate, with 3 of the top 5 and half of the top 10 most popular models for customer-facing apps.\",\n",
    "    \"70% of respondents are using RAG in some form.\",\n",
    "    \"More than 50% update their models at least monthly, with 17% doing so weekly.\",\n",
    "]\n",
    "\n",
    "# dataset\n",
    "qa_pairs = [{\"question\":q,\"answer\":a} for q,a in zip(inputs,outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# write \n",
    "csv_path = \"D:/AI_Projects/RAG/data/goldens.csv\"\n",
    "df.to_csv(csv_path,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13928196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Multi_Docs_Chats\"\n",
    "\n",
    "#store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output pairs for AgenticAIReport\",\n",
    "    \n",
    ")\n",
    "# Add examples one by one\n",
    "for q, a in zip(inputs, outputs):\n",
    "    client.create_example(\n",
    "        inputs={\"question\": q},      # must be ONE dict\n",
    "        outputs={\"answer\": a},       # must be ONE dict\n",
    "        dataset_id=dataset.id\n",
    "        \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff80372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/AI_Projects/RAG\")\n",
    "\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from multi_doc_chat.src.document_chat.retrieval import ConversationalRAG\n",
    "import os\n",
    "from multi_doc_chat.utils.model_loader import ModelLoader\n",
    "\n",
    "# simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self,file_path:str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "    \n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path :str = \"D:/AI_Projects/RAG/data/2025 AI engineering Report.txt\",\n",
    "    chunk_size: int = 1000,\n",
    "chunk_overlap: int = 200,\n",
    "k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        #build retriever\n",
    "        ingestor.built_retriver(\n",
    "        uploaded_files=[file_adapter],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        k=k\n",
    "    )\n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        loader = ModelLoader()\n",
    "        \n",
    "        # create RAG instance and load retreiver\n",
    "        \n",
    "        rag = ConversationalRAG(session_id=session_id,model_loader=loader)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "    )\n",
    "        # get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\":answer}\n",
    "    \n",
    "    except Exception as e:\n",
    "            return {\"answer\": f\"Error: {str(e)}\"}\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f434a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import-ipynb\n",
      "  Obtaining dependency information for import-ipynb from https://files.pythonhosted.org/packages/ec/62/e0b830773060d2a390aa923dcc8afc680d798bdbdadb6394f760fac62517/import_ipynb-0.2-py3-none-any.whl.metadata\n",
      "  Using cached import_ipynb-0.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting IPython (from import-ipynb)\n",
      "  Obtaining dependency information for IPython from https://files.pythonhosted.org/packages/f1/df/8ee1c5dd1e3308b5d5b2f2dfea323bb2f3827da8d654abb6642051199049/ipython-9.8.0-py3-none-any.whl.metadata\n",
      "  Using cached ipython-9.8.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting nbformat (from import-ipynb)\n",
      "  Obtaining dependency information for nbformat from https://files.pythonhosted.org/packages/a9/82/0340caa499416c78e5d8f5f05947ae4bc3cba53c9f038ab6e9ed964e22f1/nbformat-5.10.4-py3-none-any.whl.metadata\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting colorama>=0.4.4 (from IPython->import-ipynb)\n",
      "  Obtaining dependency information for colorama>=0.4.4 from https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl.metadata\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting decorator>=4.3.2 (from IPython->import-ipynb)\n",
      "  Obtaining dependency information for decorator>=4.3.2 from https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl.metadata\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ipython-pygments-lexers>=1.0.0 (from IPython->import-ipynb)\n",
      "  Obtaining dependency information for ipython-pygments-lexers>=1.0.0 from https://files.pythonhosted.org/packages/d9/33/1f075bf72b0b747cb3288d011319aaf64083cf2efef8354174e3ed4540e2/ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata\n",
      "  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi>=0.18.1 (from IPython->import-ipynb)\n",
      "  Obtaining dependency information for jedi>=0.18.1 from https://files.pythonhosted.org/packages/c0/5a/9cac0c82afec3d09ccd97c8b6502d48f165f9124db81b4bcb90b4af974ee/jedi-0.19.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in c:\\python312\\lib\\site-packages (from IPython->import-ipynb) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\python312\\lib\\site-packages (from IPython->import-ipynb) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\python312\\lib\\site-packages (from IPython->import-ipynb) (2.19.2)\n",
      "Collecting stack_data>=0.6.0 (from IPython->import-ipynb)\n",
      "  Obtaining dependency information for stack_data>=0.6.0 from https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl.metadata\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\python312\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\python312\\lib\\site-packages (from nbformat->import-ipynb) (2.21.2)\n",
      "Collecting jsonschema>=2.6 (from nbformat->import-ipynb)\n",
      "  Obtaining dependency information for jsonschema>=2.6 from https://files.pythonhosted.org/packages/bf/9c/8c95d856233c1f82500c2450b8c68576b4cf1c871db3afac5c34ff84e6fd/jsonschema-4.25.1-py3-none-any.whl.metadata\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\python312\\lib\\site-packages (from nbformat->import-ipynb) (5.9.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\python312\\lib\\site-packages (from jedi>=0.18.1->IPython->import-ipynb) (0.8.5)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat->import-ipynb)\n",
      "  Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/3a/2a/7cc015f5b9f5db42b7d48157e23356022889fc354a2813c15934b7cb5c0e/attrs-25.4.0-py3-none-any.whl.metadata\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->import-ipynb)\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/41/45/1a4ed80516f02155c51f51e8cedb3c1902296743db0bbc66608a0db2814f/jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->import-ipynb)\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/2c/58/ca301544e1fa93ed4f80d724bf5b194f6e4b945841c5bfd555878eea9fcb/referencing-0.37.0-py3-none-any.whl.metadata\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.30.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.6)\n",
      "Requirement already satisfied: wcwidth in c:\\python312\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.14)\n",
      "Collecting executing>=1.2.0 (from stack_data>=0.6.0->IPython->import-ipynb)\n",
      "  Obtaining dependency information for executing>=1.2.0 from https://files.pythonhosted.org/packages/c1/ea/53f2148663b321f21b5a606bd5f191517cf40b7072c0497d3c92c4a13b1e/executing-2.2.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data>=0.6.0->IPython->import-ipynb)\n",
      "  Obtaining dependency information for asttokens>=2.1.0 from https://files.pythonhosted.org/packages/d2/39/e7eaf1799466a4aef85b6a4fe7bd175ad2b1c6345066aa33f1f58d4b18d0/asttokens-3.0.1-py3-none-any.whl.metadata\n",
      "  Using cached asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pure-eval in c:\\python312\\lib\\site-packages (from stack_data>=0.6.0->IPython->import-ipynb) (0.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\python312\\lib\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import-ipynb) (4.15.0)\n",
      "Using cached import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n",
      "Using cached ipython-9.8.0-py3-none-any.whl (621 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-3.0.1-py3-none-any.whl (27 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: jedi, ipython-pygments-lexers, executing, decorator, colorama, attrs, asttokens, stack_data, referencing, jsonschema-specifications, IPython, jsonschema, nbformat, import-ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Python312\\\\share'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install import-ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1cc66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-12-08T12:23:56.060766Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T12:23:56.061767Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T12:23:56.064774Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"session_id\": \"session_20251208_175356_c2118f26\", \"temp_dir\": \"data\\\\session_20251208_175356_c2118f26\", \"faiss_dir\": \"faiss_index\\\\session_20251208_175356_c2118f26\", \"sessionized\": true, \"timestamp\": \"2025-12-08T12:23:56.068056Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_175356_c2118f26\\\\d85ccdf4.txt\", \"timestamp\": \"2025-12-08T12:23:56.086051Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T12:23:56.090050Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T12:23:56.093053Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T12:23:56.094048Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T12:24:48.752624Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For customer-facing applications, which company's models dominate the top rankings?\n",
      "\n",
      "Answer: Error: Error in [d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py] at line [888] | Message: Failed to build retriever\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 88, in built_retriver\n",
      "    vs= fm.load_or_create(texts=texts,metadatas=metas)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 90, in built_retriver\n",
      "    vs = fm.load_or_create(texts=texts,metadatas=metas)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the function wuth a sample question\n",
    "#from notebook.evaluation.answer_ai_report_question\n",
    " \n",
    "#import import_ipynb\n",
    "#from notebook.evaluation import answer_ai_report_question\n",
    "test_input = {\"question\": \"For customer-facing applications, which company's models dominate the top rankings?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langsmith.evaluation import evaluate ,LangChainStringEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc16249",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'faiss' has no attribute 'IndexFlatL2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexFlatL2\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'faiss' has no attribute 'IndexFlatL2'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811c40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-12-08T12:25:14.811574Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T12:25:14.813571Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T12:25:14.814577Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"session_id\": \"session_20251208_175514_10352ba9\", \"temp_dir\": \"data\\\\session_20251208_175514_10352ba9\", \"faiss_dir\": \"faiss_index\\\\session_20251208_175514_10352ba9\", \"sessionized\": true, \"timestamp\": \"2025-12-08T12:25:14.817571Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_175514_10352ba9\\\\79b807a7.txt\", \"timestamp\": \"2025-12-08T12:25:14.819571Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T12:25:14.823573Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T12:25:14.825574Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T12:25:14.827578Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the questions from the  dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T12:25:18.731106Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n",
      "{\"timestamp\": \"2025-12-08T12:25:18.736207Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T12:25:18.738102Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T12:25:18.739097Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"session_id\": \"session_20251208_175518_3152a3c4\", \"temp_dir\": \"data\\\\session_20251208_175518_3152a3c4\", \"faiss_dir\": \"faiss_index\\\\session_20251208_175518_3152a3c4\", \"sessionized\": true, \"timestamp\": \"2025-12-08T12:25:18.751130Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_175518_3152a3c4\\\\85668c3f.txt\", \"timestamp\": \"2025-12-08T12:25:18.753201Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T12:25:18.757099Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T12:25:18.759102Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T12:25:18.761103Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1:For customer-facing applications, which company's models dominate the top rankings?\n",
      "A1:Error: Error in [d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py] at line [888] | Message: Failed to build retriever\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 88, in built_retriver\n",
      "    vs= fm.load_or_create(texts=texts,metadatas=metas)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 90, in built_retriver\n",
      "    vs = fm.load_or_create(texts=texts,metadatas=metas)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T12:25:22.956650Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n",
      "{\"timestamp\": \"2025-12-08T12:25:22.962648Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T12:25:22.963648Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T12:25:22.964648Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"session_id\": \"session_20251208_175522_1de60844\", \"temp_dir\": \"data\\\\session_20251208_175522_1de60844\", \"faiss_dir\": \"faiss_index\\\\session_20251208_175522_1de60844\", \"sessionized\": true, \"timestamp\": \"2025-12-08T12:25:22.966651Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_175522_1de60844\\\\e4737d98.txt\", \"timestamp\": \"2025-12-08T12:25:22.978873Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T12:25:22.982873Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T12:25:22.983873Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T12:25:22.985873Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2:What percentage of respondents are using RAG in some form?\n",
      "A2:Error: Error in [d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py] at line [888] | Message: Failed to build retriever\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 88, in built_retriver\n",
      "    vs= fm.load_or_create(texts=texts,metadatas=metas)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 90, in built_retriver\n",
      "    vs = fm.load_or_create(texts=texts,metadatas=metas)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T12:25:27.102762Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3:How often are most respondents updating their models?\n",
      "A3:Error: Error in [d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py] at line [888] | Message: Failed to build retriever\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 88, in built_retriver\n",
      "    vs= fm.load_or_create(texts=texts,metadatas=metas)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 90, in built_retriver\n",
      "    vs = fm.load_or_create(texts=texts,metadatas=metas)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI_Projects/RAG\\multi_doc_chat\\src\\document_ingestion\\data_ingestion.py\", line 178, in load_or_create\n",
      "    self.vs = FAISS.from_texts(texts=texts, embedding=self.emb, metadatas=metadatas or [])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 931, in from_texts\n",
      "    return cls.__from(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 888, in __from\n",
      "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'IndexFlatL2'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Test with all golden questions\n",
    "print(\"Testing all the questions from the  dataset\")\n",
    "for i, q in enumerate(inputs,1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}:{q}\")\n",
    "    print(f\"A{i}:{result['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9400dc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langsmith\n",
      "  Obtaining dependency information for langsmith from https://files.pythonhosted.org/packages/b8/6f/d5f9c4f1e03c91045d3675dc99df0682bc657952ad158c92c1f423de04f4/langsmith-0.4.56-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.4.56-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith)\n",
      "  Obtaining dependency information for orjson>=3.9.14 from https://files.pythonhosted.org/packages/d4/fb/f05646c43d5450492cb387de5549f6de90a71001682c17882d9f66476af5/orjson-3.11.5-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading orjson-3.11.5-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.7/42.7 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\python312\\lib\\site-packages (from langsmith) (25.0)\n",
      "Collecting pydantic<3,>=1 (from langsmith)\n",
      "  Obtaining dependency information for pydantic<3,>=1 from https://files.pythonhosted.org/packages/5a/87/b70ad306ebb6f9b585f114d0ac2137d792b48be34d732d60e597c2f8465a/pydantic-2.12.5-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "     ---------------------------------------- 0.0/90.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.6/90.6 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith)\n",
      "  Obtaining dependency information for requests-toolbelt>=1.0.0 from https://files.pythonhosted.org/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests>=2.0.0 (from langsmith)\n",
      "  Obtaining dependency information for requests>=2.0.0 from https://files.pythonhosted.org/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langsmith)\n",
      "  Obtaining dependency information for uuid-utils<1.0,>=0.12.0 from https://files.pythonhosted.org/packages/38/92/41c8734dd97213ee1d5ae435cf4499705dc4f2751e3b957fd12376f61784/uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith)\n",
      "  Obtaining dependency information for zstandard>=0.23.0 from https://files.pythonhosted.org/packages/79/3b/fa54d9015f945330510cb5d0b0501e8253c127cca7ebe8ba46a965df18c5/zstandard-0.25.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith)\n",
      "  Obtaining dependency information for anyio from https://files.pythonhosted.org/packages/7f/9c/36c5c37947ebfb8c7f22e0eb6e4d188ee2d53aa3880f3f2744fb894f0cb1/anyio-4.12.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->langsmith)\n",
      "  Obtaining dependency information for certifi from https://files.pythonhosted.org/packages/70/7d/9bc192684cea499815ff478dfcdc13835ddf401365057044fb721ec6bddb/certifi-2025.11.12-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith)\n",
      "  Obtaining dependency information for h11>=0.16 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1->langsmith)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1->langsmith)\n",
      "  Obtaining dependency information for pydantic-core==2.41.5 from https://files.pythonhosted.org/packages/86/45/00173a033c801cacf67c190fef088789394feaf88a98a7035b0e40d53dc9/pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\python312\\lib\\site-packages (from pydantic<3,>=1->langsmith) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1->langsmith)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.2 from https://files.pythonhosted.org/packages/dc/9b/47798a6c91d8bdb567fe2698fe81e0c6b7cb7ef4d13da4114b41d239f65d/typing_inspection-0.4.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.0.0->langsmith) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.0.0->langsmith) (2.5.0)\n",
      "Downloading langsmith-0.4.56-py3-none-any.whl (411 kB)\n",
      "   ---------------------------------------- 0.0/411.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 411.8/411.8 kB 13.0 MB/s eta 0:00:00\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "   ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 73.5/73.5 kB ? eta 0:00:00\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.8/78.8 kB ? eta 0:00:00\n",
      "Downloading orjson-3.11.5-cp312-cp312-win_amd64.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.3/133.3 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "   ---------------------------------------- 0.0/463.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 463.6/463.6 kB 14.2 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.1/2.0 MB 34.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 21.4 MB/s eta 0:00:00\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.5/54.5 kB ? eta 0:00:00\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 183.0/183.0 kB ? eta 0:00:00\n",
      "Downloading zstandard-0.25.0-cp312-cp312-win_amd64.whl (506 kB)\n",
      "   ---------------------------------------- 0.0/506.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 506.3/506.3 kB 33.1 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "   ---------------------------------------- 0.0/113.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 113.4/113.4 kB ? eta 0:00:00\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: zstandard, uuid-utils, typing-inspection, pydantic-core, orjson, h11, certifi, anyio, annotated-types, requests, pydantic, httpcore, requests-toolbelt, httpx, langsmith\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python312\\\\Scripts\\\\httpx.exe' -> 'C:\\\\Python312\\\\Scripts\\\\httpx.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langsmith\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0284898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(target: 'Union[TARGET_T, Runnable, EXPERIMENT_T, tuple[EXPERIMENT_T, EXPERIMENT_T]]', /, data: 'Optional[DATA_T]' = None, evaluators: 'Optional[Union[Sequence[EVALUATOR_T], Sequence[COMPARATIVE_EVALUATOR_T]]]' = None, summary_evaluators: 'Optional[Sequence[SUMMARY_EVALUATOR_T]]' = None, metadata: 'Optional[dict]' = None, experiment_prefix: 'Optional[str]' = None, description: 'Optional[str]' = None, max_concurrency: 'Optional[int]' = 0, num_repetitions: 'int' = 1, client: 'Optional[langsmith.Client]' = None, blocking: 'bool' = True, experiment: 'Optional[EXPERIMENT_T]' = None, upload_results: 'bool' = True, error_handling: \"Literal['log', 'ignore']\" = 'log', **kwargs: 'Any') -> 'Union[ExperimentResults, ComparativeExperimentResults]'\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from langsmith.evaluation import evaluate\n",
    "print(inspect.signature(evaluate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f5c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_Projects\\RAG\\.venv\\Scripts\\python.exe: No module named uv\n"
     ]
    }
   ],
   "source": [
    "uv pip install --upgrade langsmith langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8dc0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langsmith.evaluation.criteria'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangsmith\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangsmith\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcriteria\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CriteriaEvaluator\n\u001b[32m      4\u001b[39m qa_evaluator = CriteriaEvaluator(\u001b[33m\"\u001b[39m\u001b[33mcorrectness\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m dataset_name = \u001b[33m\"\u001b[39m\u001b[33mAgenticAIReportGoldens\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langsmith.evaluation.criteria'"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.evaluators import CriteriaEvaluator\n",
    "\n",
    "\n",
    "qa_evaluator = CriteriaEvaluator(\"correctness\")\n",
    "dataset_name = \"AgenticAIReportGoldens\"\n",
    "\n",
    "# Run evaluation using our RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-agenticAIReport-qa-rag\",\n",
    "    # Experiment metadata\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ea157ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\AI_Projects\\RAG\\.venv\\Scripts\\python.exe\n",
      "['_expect', '_internal', 'anonymizer', 'async_client', 'beta', 'client', 'env', 'evaluation', 'middleware', 'pytest_plugin', 'run_helpers', 'run_trees', 'schemas', 'testing', 'utils', 'uuid', 'wrappers']\n"
     ]
    }
   ],
   "source": [
    "#from langsmith.evaluation import evaluate\n",
    "#import inspect\n",
    "#print(inspect.getsource(evaluate))\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import langsmith\n",
    "import pkgutil\n",
    "\n",
    "modules = [m.name for m in pkgutil.iter_modules(langsmith.__path__)]\n",
    "print(modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a8c493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\AI_Projects\\RAG\\.venv\\Lib\\site-packages\\langsmith\n",
      "0.4.56\n"
     ]
    }
   ],
   "source": [
    "import langsmith, inspect, os\n",
    "print(os.path.dirname(inspect.getfile(langsmith)))\n",
    "print(langsmith.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ab0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uv pip install --force-reinstall --no-cache-dir \"langsmith[all]\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4753581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "#from langsmith.beta.evaluation import LLMCriteriaEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79129cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.20\n",
      "0.1.58\n",
      "2.12.0\n",
      "2.41.1\n"
     ]
    }
   ],
   "source": [
    "import langchain, langsmith, pydantic, pydantic_core\n",
    "\n",
    "print(langchain.__version__)        # 0.1.20\n",
    "print(langsmith.__version__)        # 0.1.58\n",
    "print(pydantic.__version__)         # 2.12.0\n",
    "print(pydantic_core.__version__)    # 2.41.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba196d",
   "metadata": {},
   "source": [
    "# Custom Correctness Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871d1d2",
   "metadata": {},
   "source": [
    "##### creating llm as-a-judge evaluator to assess semantic and factual aligment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b716aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    Custom LLM-as-a-Judge evaluator for correctness.\n",
    "    \n",
    "    Correctness means how well the actual model output matches the reference output \n",
    "    in terms of factual accuracy, coverage, and meaning.\n",
    "    \n",
    "    Args:\n",
    "        run: The Run object containing the actual outputs\n",
    "        example: The Example object containing the expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'score' (1 for correct, 0 for incorrect) and 'reasoning'\n",
    "    \"\"\"\n",
    "    # Extract actual and expected outputs\n",
    "    actual_output = run.outputs.get(\"answer\", \"\")\n",
    "    expected_output = example.outputs.get(\"answer\", \"\")\n",
    "    input_question = example.inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"\"\"You are an evaluator whose job is to judge correctness.\n",
    "    Correctness means how well the actual model output matches the reference output in terms of factual accuracy, coverage, and meaning.\n",
    "    - If the actual output matches the reference output semantically (even if wording differs), it should be marked correct.\n",
    "    - If the output misses key facts, introduces contradictions, or is factually incorrect, it should be marked incorrect.\n",
    "    Do not penalize for stylistic or formatting differences unless they change meaning.\"\"\"),\n",
    "           (\"human\", \"\"\"<example>\n",
    "    <input>\n",
    "    {input}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "    Expected Output: {expected_output}\n",
    "\n",
    "    Actual Output: {actual_output}\n",
    "    </output>\n",
    "    </example>\n",
    "\n",
    "    Please grade the following agent run given the input, expected output, and actual output.\n",
    "    Focus only on correctness (semantic and factual alignment).\n",
    "\n",
    "    Respond with:\n",
    "    1. A brief reasoning (1-2 sentences)\n",
    "    2. A final verdict: either \"CORRECT\" or \"INCORRECT\"\n",
    "\n",
    "    Format your response as:\n",
    "    Reasoning: [your reasoning]\n",
    "    Verdict: [CORRECT or INCORRECT]\"\"\")])\n",
    "    \n",
    "    ## initialize llm using groq llm\n",
    "    \n",
    "\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Create chain and invoke\n",
    "    chain = eval_prompt | llm \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"input\": input_question,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"actual_output\": actual_output\n",
    "        })\n",
    "        \n",
    "        response_text = response.content\n",
    "        \n",
    "        # Parse the response\n",
    "        reasoning = \"\"\n",
    "        verdict = \"\"\n",
    "        \n",
    "        for line in response_text(\"\\n\"):\n",
    "             if line.startswith(\"Reasoning:\"):\n",
    "                    reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "             elif line.startswith(\"Verdict:\"):\n",
    "                verdict = line.replace(\"Verdict:\", \"\").strip()\n",
    "                \n",
    "        # Convert verdict to score (1 for correct, 0 for incorrect)\n",
    "        score = 1 if \"CORRECT\" in verdict.upper() else 0\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"comment\": f\"Verdict: {verdict}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Error during evaluation: {str(e)}\"\n",
    "        }\n",
    "        \n",
    "    \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207b104",
   "metadata": {},
   "source": [
    "### Run evaluation with custome correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61375b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'agenticAIReport-correctness-eval-bcf92f39' at:\n",
      "https://smith.langchain.com/o/da78cfb0-6927-449d-96ba-935b2ed752c9/datasets/681ed8e0-1369-45f0-a017-e645d9dbe879/compare?selectedSessions=fd9285dd-42c0-4fce-b5d6-d6174d888d92\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]{\"timestamp\": \"2025-12-08T14:47:11.263759Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"timestamp\": \"2025-12-08T14:47:11.269297Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T14:47:11.270288Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"timestamp\": \"2025-12-08T14:47:11.272880Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from environment variable\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T14:47:11.274042Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T14:47:11.278040Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1g...\"}, \"timestamp\": \"2025-12-08T14:47:11.278040Z\", \"level\": \"info\", \"event\": \"API key loaded successfully\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T14:47:11.280041Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"config_path\": \"D:\\\\AI_Projects\\\\RAG\\\\multi_doc_chat\\\\config\\\\config.yaml\", \"timestamp\": \"2025-12-08T14:47:11.282568Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "{\"session_id\": \"session_20251208_201711_e630650e\", \"temp_dir\": \"data\\\\session_20251208_201711_e630650e\", \"faiss_dir\": \"faiss_index\\\\session_20251208_201711_e630650e\", \"sessionized\": true, \"timestamp\": \"2025-12-08T14:47:11.286579Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"session_id\": \"session_20251208_201711_f4ac5144\", \"temp_dir\": \"data\\\\session_20251208_201711_f4ac5144\", \"faiss_dir\": \"faiss_index\\\\session_20251208_201711_f4ac5144\", \"sessionized\": true, \"timestamp\": \"2025-12-08T14:47:11.288605Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"session_id\": \"session_20251208_201711_4393c7c7\", \"temp_dir\": \"data\\\\session_20251208_201711_4393c7c7\", \"faiss_dir\": \"faiss_index\\\\session_20251208_201711_4393c7c7\", \"sessionized\": true, \"timestamp\": \"2025-12-08T14:47:11.289576Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_201711_e630650e\\\\259675e7.txt\", \"timestamp\": \"2025-12-08T14:47:11.293679Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_201711_f4ac5144\\\\8460812d.txt\", \"timestamp\": \"2025-12-08T14:47:11.296681Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"uploaded\": \"2025 AI engineering Report.txt\", \"saved_as\": \"data\\\\session_20251208_201711_4393c7c7\\\\5bc3a4d7.txt\", \"timestamp\": \"2025-12-08T14:47:11.297690Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T14:47:11.299752Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T14:47:11.300396Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T14:47:11.305424Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-12-08T14:47:11.312630Z\", \"level\": \"info\", \"event\": \"Dcouments loaded\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T14:47:11.314763Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T14:47:11.316744Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"chunks\": 4, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-12-08T14:47:11.317965Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T14:47:11.329587Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"timestamp\": \"2025-12-08T14:47:11.335146Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T14:47:16.033791Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n",
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T14:47:16.473495Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n",
      "{\"error\": \"module 'faiss' has no attribute 'IndexFlatL2'\", \"timestamp\": \"2025-12-08T14:47:19.228771Z\", \"level\": \"error\", \"event\": \"Failed to build retriever\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2it [00:13,  5.61s/it]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "3it [00:32, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed! Check the LangSmith UI for detailed results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with the custom correctness evaluator\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluators - using custom correctness evaluator\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "dataset_name = \"Multi_Docs_Chats\"\n",
    "\n",
    " # Run evaluation\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data = dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"agenticAIReport-correctness-eval\",\n",
    "    description=\"Evaluating RAG system with custom correctness evaluator (LLM-as-a-Judge)\",\n",
    "     metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"evaluator\": \"custom_correctness_llm_judge\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n",
    "print(\"\\nEvaluation completed! Check the LangSmith UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d6283",
   "metadata": {},
   "source": [
    "### combine multiple evaluators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e4a25",
   "metadata": {},
   "source": [
    "##### you can use multiple  evaluators together to get different perspectives on your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Combine custom correctness evaluator with LangChain's built-in evaluators\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Combine custom and built-in evaluators\n",
    "combined_evaluators = [\n",
    "    correctness_evaluator,  # Custom LLM-as-a-Judge\n",
    "    LangChainStringEvaluator(\"cot_qa\"),  # Chain-of-thought QA evaluator\n",
    "]\n",
    "\n",
    "# Run evaluation with multiple evaluators\n",
    "# Uncomment to run:\n",
    "# experiment_results_combined = evaluate(\n",
    "#     answer_ai_report_question,\n",
    "#     data=dataset_name,\n",
    "#     evaluators=combined_evaluators,\n",
    "#     experiment_prefix=\"agenticAIReport-multi-eval\",\n",
    "#     description=\"Evaluating RAG system with multiple evaluators\",\n",
    "#     metadata={\n",
    "#         \"variant\": \"RAG with FAISS\",\n",
    "#         \"evaluators\": \"correctness + cot_qa\",\n",
    "#         \"chunk_size\": 1000,\n",
    "#         \"chunk_overlap\": 200,\n",
    "#         \"k\": 5,\n",
    "#     },\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
