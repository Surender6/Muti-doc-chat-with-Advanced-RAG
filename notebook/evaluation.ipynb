{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#QA\n",
    "inputs = [\n",
    "    \"For customer-facing applications, which company's models dominate the top rankings?\",\n",
    "    \"What percentage of respondents are using RAG in some form?\",\n",
    "    \"How often are most respondents updating their models?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"OpenAI models dominate, with 3 of the top 5 and half of the top 10 most popular models for customer-facing apps.\",\n",
    "    \"70% of respondents are using RAG in some form.\",\n",
    "    \"More than 50% update their models at least monthly, with 17% doing so weekly.\",\n",
    "]\n",
    "\n",
    "# dataset\n",
    "qa_pairs = [{\"question\":q,\"answer\":a} for q,a in zip(inputs,outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# write \n",
    "csv_path = \"D:/AI_Projects/RAG/data/goldens.csv\"\n",
    "df.to_csv(csv_path,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13928196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Multi_Docs_Chats\"\n",
    "\n",
    "#store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output pairs for AgenticAIReport\",\n",
    "    \n",
    ")\n",
    "# Add examples one by one\n",
    "for q, a in zip(inputs, outputs):\n",
    "    client.create_example(\n",
    "        inputs={\"question\": q},      # must be ONE dict\n",
    "        outputs={\"answer\": a},       # must be ONE dict\n",
    "        dataset_id=dataset.id\n",
    "        \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff80372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/AI_Projects/RAG\")\n",
    "\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from multi_doc_chat.src.document_chat.retrieval import ConversationalRAG\n",
    "import os\n",
    "from multi_doc_chat.utils.model_loader import ModelLoader\n",
    "\n",
    "# simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self,file_path:str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "    \n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path :str = \"D:/AI_Projects/RAG/data/2025 AI engineering Report.txt\",\n",
    "    chunk_size: int = 1000,\n",
    "chunk_overlap: int = 200,\n",
    "k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        #build retriever\n",
    "        ingestor.built_retriver(\n",
    "        uploaded_files=[file_adapter],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        k=k\n",
    "    )\n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        loader = ModelLoader()\n",
    "        \n",
    "        # create RAG instance and load retreiver\n",
    "        \n",
    "        rag = ConversationalRAG(session_id=session_id,model_loader=loader)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "    )\n",
    "        # get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\":answer}\n",
    "    \n",
    "    except Exception as e:\n",
    "            return {\"answer\": f\"Error: {str(e)}\"}\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function wuth a sample question\n",
    "#from notebook.evaluation.answer_ai_report_question\n",
    " \n",
    "#import import_ipynb\n",
    "#from notebook.evaluation import answer_ai_report_question\n",
    "test_input = {\"question\": \"For customer-facing applications, which company's models dominate the top rankings?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langsmith.evaluation import evaluate ,LangChainStringEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test with all golden questions\n",
    "print(\"Testing all the questions from the  dataset\")\n",
    "for i, q in enumerate(inputs,1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}:{q}\")\n",
    "    print(f\"A{i}:{result['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.evaluators import CriteriaEvaluator\n",
    "\n",
    "\n",
    "qa_evaluator = CriteriaEvaluator(\"correctness\")\n",
    "dataset_name = \"AgenticAIReportGoldens\"\n",
    "\n",
    "# Run evaluation using our RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-agenticAIReport-qa-rag\",\n",
    "    # Experiment metadata\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba196d",
   "metadata": {},
   "source": [
    "# Custom Correctness Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871d1d2",
   "metadata": {},
   "source": [
    "##### creating llm as-a-judge evaluator to assess semantic and factual aligment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    Custom LLM-as-a-Judge evaluator for correctness.\n",
    "    \n",
    "    Correctness means how well the actual model output matches the reference output \n",
    "    in terms of factual accuracy, coverage, and meaning.\n",
    "    \n",
    "    Args:\n",
    "        run: The Run object containing the actual outputs\n",
    "        example: The Example object containing the expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'score' (1 for correct, 0 for incorrect) and 'reasoning'\n",
    "    \"\"\"\n",
    "    # Extract actual and expected outputs\n",
    "    actual_output = run.outputs.get(\"answer\", \"\")\n",
    "    expected_output = example.outputs.get(\"answer\", \"\")\n",
    "    input_question = example.inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"\"\"You are an evaluator whose job is to judge correctness.\n",
    "    Correctness means how well the actual model output matches the reference output in terms of factual accuracy, coverage, and meaning.\n",
    "    - If the actual output matches the reference output semantically (even if wording differs), it should be marked correct.\n",
    "    - If the output misses key facts, introduces contradictions, or is factually incorrect, it should be marked incorrect.\n",
    "    Do not penalize for stylistic or formatting differences unless they change meaning.\"\"\"),\n",
    "           (\"human\", \"\"\"<example>\n",
    "    <input>\n",
    "    {input}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "    Expected Output: {expected_output}\n",
    "\n",
    "    Actual Output: {actual_output}\n",
    "    </output>\n",
    "    </example>\n",
    "\n",
    "    Please grade the following agent run given the input, expected output, and actual output.\n",
    "    Focus only on correctness (semantic and factual alignment).\n",
    "\n",
    "    Respond with:\n",
    "    1. A brief reasoning (1-2 sentences)\n",
    "    2. A final verdict: either \"CORRECT\" or \"INCORRECT\"\n",
    "\n",
    "    Format your response as:\n",
    "    Reasoning: [your reasoning]\n",
    "    Verdict: [CORRECT or INCORRECT]\"\"\")])\n",
    "    \n",
    "    ## initialize llm using groq llm\n",
    "    \n",
    "\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Create chain and invoke\n",
    "    chain = eval_prompt | llm \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"input\": input_question,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"actual_output\": actual_output\n",
    "        })\n",
    "        \n",
    "        response_text = response.content\n",
    "        \n",
    "        # Parse the response\n",
    "        reasoning = \"\"\n",
    "        verdict = \"\"\n",
    "        \n",
    "        for line in response_text(\"\\n\"):\n",
    "             if line.startswith(\"Reasoning:\"):\n",
    "                    reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "             elif line.startswith(\"Verdict:\"):\n",
    "                verdict = line.replace(\"Verdict:\", \"\").strip()\n",
    "                \n",
    "        # Convert verdict to score (1 for correct, 0 for incorrect)\n",
    "        score = 1 if \"CORRECT\" in verdict.upper() else 0\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"comment\": f\"Verdict: {verdict}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Error during evaluation: {str(e)}\"\n",
    "        }\n",
    "        \n",
    "    \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207b104",
   "metadata": {},
   "source": [
    "### Run evaluation with custome correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61375b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with the custom correctness evaluator\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluators - using custom correctness evaluator\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "dataset_name = \"Multi_Docs_Chats\"\n",
    "\n",
    " # Run evaluation\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data = dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"agenticAIReport-correctness-eval\",\n",
    "    description=\"Evaluating RAG system with custom correctness evaluator (LLM-as-a-Judge)\",\n",
    "     metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"evaluator\": \"custom_correctness_llm_judge\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n",
    "print(\"\\nEvaluation completed! Check the LangSmith UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d6283",
   "metadata": {},
   "source": [
    "### combine multiple evaluators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e4a25",
   "metadata": {},
   "source": [
    "##### you can use multiple  evaluators together to get different perspectives on your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Combine custom correctness evaluator with LangChain's built-in evaluators\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Combine custom and built-in evaluators\n",
    "combined_evaluators = [\n",
    "    correctness_evaluator,  # Custom LLM-as-a-Judge\n",
    "    LangChainStringEvaluator(\"cot_qa\"),  # Chain-of-thought QA evaluator\n",
    "]\n",
    "\n",
    "# Run evaluation with multiple evaluators\n",
    "# Uncomment to run:\n",
    "# experiment_results_combined = evaluate(\n",
    "#     answer_ai_report_question,\n",
    "#     data=dataset_name,\n",
    "#     evaluators=combined_evaluators,\n",
    "#     experiment_prefix=\"agenticAIReport-multi-eval\",\n",
    "#     description=\"Evaluating RAG system with multiple evaluators\",\n",
    "#     metadata={\n",
    "#         \"variant\": \"RAG with FAISS\",\n",
    "#         \"evaluators\": \"correctness + cot_qa\",\n",
    "#         \"chunk_size\": 1000,\n",
    "#         \"chunk_overlap\": 200,\n",
    "#         \"k\": 5,\n",
    "#     },\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
